
In this paper we discuss our participation to
the 2013 Semeval Semantic Textual Similarity
task. Our core features include (i) a set of met-
rics borrowed from automatic machine trans-
lation, originally intended to evaluate auto-
matic against reference translations and (ii) an
instance of explicit semantic analysis, built
upon opening paragraphs of Wikipedia 2010
articles. Our similarity estimator relies on a
support vector regressor with RBF kernel. Our
best approach required 13 machine transla-
tion metrics + explicit semantic analysis and
ranked 65 in the competition. Our post-
competition analysis shows that the features
have a good expression level, but overfitting
and ?mainly? normalization issues caused
our correlation values to decrease.
1 