
Much natural language processing still depends on
the Euclidean (cosine) distance function between
two feature vectors, but this has severe problems
with regard to feature weightings and feature cor-
relations. To answer these problems, we propose an
optimal metric distance that can be used as an alter-
native to the cosine distance, thus accommodating
the two problems at the same time. This metric is
optimal in the sense of global quadratic minimiza-
tion, and can be obtained from the clusters in the
training data in a supervised fashion.
We confirmed the effect of the proposed metric
distance by a synonymous sentence retrieval task,
document retrieval task and the K-means clustering
of general vectorial data. The results showed con-
stant improvement over the baseline method of Eu-
clid and tf.idf, and were especially prominent for
the sentence retrieval task, showing a 33% increase
in the 11-point average precision.
1 